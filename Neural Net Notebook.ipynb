{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activations (ϕ)\n",
    "function ReLU(x::T)::T where T<:Real \n",
    "    return max(x, 0)\n",
    "end\n",
    "\n",
    "function LeakyReLU(x::T)::T where T<:Real \n",
    "    return max(x, 0.01x)\n",
    "end\n",
    "\n",
    "function sigmoid(x::T)::T where T<:Real \n",
    "    return 1.0 / (1 + exp(-x))\n",
    "end\n",
    "\n",
    "# Derivatives\n",
    "function ∇ReLU(x::T)::T where T<:Real \n",
    "    return x > 0\n",
    "end\n",
    "\n",
    "function ∇LeakyReLU(x::T)::T where T<:Real \n",
    "    return x < 0 ? 0.01 : 1.0\n",
    "end\n",
    "\n",
    "function ∇sigmoid(x::T)::T where T<:Real\n",
    "    y = sigmoid(x)\n",
    "    return y * (1 - y)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gradient (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function gradient(f::Function)\n",
    "    if f == ReLU\n",
    "        ∇f = ∇ReLU\n",
    "    elseif f == LeakyReLU\n",
    "        ∇f = ∇LeakyReLU\n",
    "    elseif f == sigmoid\n",
    "        ∇f = ∇sigmoid\n",
    "    elseif f == mse\n",
    "        ∇f = ∇mse\n",
    "    end\n",
    "    \n",
    "    return ∇f\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "∇mse (generic function with 3 methods)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Error Calculations\n",
    "\n",
    "# Mean Squared Error\n",
    "function mse(x::T, target::T) where T<:Real\n",
    "    return .5(target-x)^2\n",
    "end\n",
    "\n",
    "function mse(xs::Vector{T}, target::T) where T<:Real \n",
    "    err(x) = target - x\n",
    "    return sum(err.(xs).^2)/2*length(xs)\n",
    "end\n",
    "\n",
    "function mse(xs::Vector{T}, target::Vector{T}) where T<:Real\n",
    "    sum((xs .- target).^2)/2*length(xs)\n",
    "end\n",
    "\n",
    "# Derivatives\n",
    "function ∇mse(x::T, target::T) where T<:Real\n",
    "    return x - target\n",
    "end\n",
    "\n",
    "function ∇mse(xs::Vector{T}, target::T) where T<:Real\n",
    "    return xs .- target\n",
    "end\n",
    "\n",
    "function ∇mse(xs::Vector{T}, target::Vector{T}) where T<:Real\n",
    "    return xs .- target\n",
    "end    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type Layer end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct FeedForwardLayer <: Layer\n",
    "    neurons::Array\n",
    "    bias::Array\n",
    "    \n",
    "    ϕ::Function\n",
    "    ∇ϕ::Function\n",
    "    \n",
    "    input::Array\n",
    "    net::Array\n",
    "    output::Array\n",
    "      \n",
    "    function FeedForwardLayer(input_dim::Int, output_dim::Int, ϕ::Function)\n",
    "        neurons = randn(input_dim, output_dim)\n",
    "        bias = randn(1, output_dim)\n",
    "        return new(neurons, bias, ϕ, gradient(ϕ))\n",
    "    end\n",
    "    \n",
    "    function FeedForwardLayer(neurons::Array, bias::Vector, ϕ::Function)\n",
    "        return new(neurons, bias, ϕ, gradient(ϕ))\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct FeedForwardOutputLayer <: Layer\n",
    "    neurons::Array\n",
    "    bias::Array\n",
    "    \n",
    "    ϕ::Function\n",
    "    ∇ϕ::Function\n",
    "    \n",
    "    input::Array\n",
    "    net::Array\n",
    "    output::Array\n",
    "      \n",
    "    function FeedForwardOutputLayer(input_dim::Int, output_dim::Int, ϕ::Function)\n",
    "        neurons = randn(input_dim, output_dim)\n",
    "        bias = zeros(1, output_dim)\n",
    "        return new(neurons, bias, ϕ, gradient(ϕ))\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type NeuralNetwork end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct FeedForwardNeuralNetwork <: NeuralNetwork\n",
    "    layers::Array{Layer}\n",
    "    η::Float64\n",
    "    \n",
    "    loss::Function\n",
    "    ∇loss::Function\n",
    "    \n",
    "    function FeedForwardNeuralNetwork(input_dim::Int, hidden_dims::Vector, output_dim::Int, ϕ::Vector{Function}, loss::Function, η=0.01)\n",
    "        layers = []\n",
    "        \n",
    "        push!(layers, FeedForwardLayer(input_dim, hidden_dims[1], ϕ[1]))\n",
    "        \n",
    "        for i in 1:length(hidden_dims)-1\n",
    "            push!(layers, FeedForwardLayer(hidden_dims[i], hidden_dims[i+1], ϕ[i+1]))\n",
    "        end\n",
    "        \n",
    "        push!(layers, FeedForwardOutputLayer(hidden_dims[end], output_dim, ϕ[end]))\n",
    "        \n",
    "        return new(layers, η, loss, gradient(loss))\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function predict(model::FeedForwardNeuralNetwork, data)\n",
    "    for layer in model.layers\n",
    "        data = data * layer.neurons .+ layer.bias\n",
    "        data = layer.ϕ.(data)\n",
    "    end\n",
    "    return data[:,1]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 3 methods)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train(model::FeedForwardNeuralNetwork, data, target, epochs=1, clear=true)\n",
    "    for i in 1:epochs\n",
    "        forwardpass(model, data)\n",
    "        backprop(model, target)\n",
    "    end\n",
    "    if clear\n",
    "        for i in 1:length(model.layers)\n",
    "            model.layers[i].input = []\n",
    "            model.layers[i].net = []\n",
    "            model.layers[i].output = []\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forwardpass (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function forwardpass(model::FeedForwardNeuralNetwork, data)\n",
    "    for layer in model.layers \n",
    "        layer.input = data\n",
    "        layer.net = data * layer.neurons .+ layer.bias\n",
    "        layer.output = layer.ϕ.(layer.net)\n",
    "        data = layer.output\n",
    "    end\n",
    "    return data\n",
    "end        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backprop (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function backprop(model::FeedForwardNeuralNetwork, target)    \n",
    "    # Work backwards from output layer\n",
    "    layer = model.layers[end]\n",
    "    # Calculate partial derivative of error with respect to each weight\n",
    "    # ∂E/∂wᵢⱼ = ∂E/∂oⱼ * ∂oⱼ/∂netⱼ * ∂netⱼ/∂wᵢⱼ\n",
    "    # Partial derivative of loss\n",
    "    ∂E_∂o = model.∇loss.(layer.output, target)\n",
    "    # Partial derivative of activated output\n",
    "    ∂o_∂net = layer.∇ϕ.(layer.net)\n",
    "    # δ=∂E/∂net\n",
    "    # Error with respect to net -- the error terms\n",
    "    δ = ∂E_∂o .* ∂o_∂net\n",
    "    # ∂net/∂w is equal to the previous layer's output (https://bit.ly/backproperror)\n",
    "    ∂net_∂w = layer.input'\n",
    "    # Calculate delta terms for the neurons and adjust by the learning rate\n",
    "    η = model.η\n",
    "    Δw = -η * ∂net_∂w * δ\n",
    "    # Update the weights of the output layer\n",
    "    layer.neurons += Δw\n",
    "    # Output layer has no bias, so no need to update it\n",
    "    # Now do the rest of the layers in reverse order\n",
    "    for i in length(model.layers)-1:-1:1\n",
    "        layer = model.layers[i]\n",
    "        # Error term of this layer is based on the error of the following layer\n",
    "        ∂E_∂o = δ * model.layers[i+1].neurons'\n",
    "        # The next part is the same\n",
    "        ∂o_∂net = layer.∇ϕ.(layer.net)\n",
    "        δ = ∂E_∂o .* ∂o_∂net\n",
    "        ∂net_∂w = layer.input' \n",
    "        Δw = -η * ∂net_∂w * δ\n",
    "        # Update the neurons\n",
    "        layer.neurons += Δw\n",
    "        # Update the bias\n",
    "        layer.bias = layer.bias .+ (-η * δ)\n",
    "    end   \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fit (generic function with 2 methods)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function fit(model::FeedForwardNeuralNetwork, data::Array{T}, target::Vector{T}, epochs::Int, showtime=false) where T<:Real\n",
    "    prediction = predict(model, data)\n",
    "    @show loss(prediction, target)\n",
    "    print(\"Training for \", epochs, \" epochs.\")\n",
    "    if showtime\n",
    "        @time train(model, data, target, epochs)\n",
    "    else\n",
    "        train(model, data, target, epochs)\n",
    "    end\n",
    "    prediction = predict(model, data)\n",
    "    @show loss(prediction, target)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss(prediction, target) = 115.38295454406902\n",
      "Training for 100000 epochs. 30.526745 seconds (24.80 M allocations: 68.584 GiB, 7.05% gc time)\n",
      "loss(prediction, target) = 0.017017430486698907\n"
     ]
    }
   ],
   "source": [
    "inputsize = 32\n",
    "hidden_layers = [64,64,32,32,16,16,8,4,2]\n",
    "output_size = 1\n",
    "activations = vcat([LeakyReLU],repeat([sigmoid], length(hidden_layers)+1))\n",
    "loss=mse;\n",
    "\n",
    "m = FeedForwardNeuralNetwork(inputsize, hidden_layers, output_size, activations, loss);\n",
    "\n",
    "samples = 32\n",
    "v = randn(samples, inputsize)\n",
    "t = rand([0.,1.], samples)\n",
    "\n",
    "fit(m, v, t, 100000, true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.1",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
