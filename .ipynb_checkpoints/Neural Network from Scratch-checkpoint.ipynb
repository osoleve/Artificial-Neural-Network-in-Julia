{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network from Scratch\n",
    "No libraries. ðŸ˜Ž\n",
    "\n",
    "WIP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Layers\n",
    "The most basic type of layer in artificial neural networks is the fully connected, or dense, layer in which every neuron in one layer is connected to every neuron in the next layer. Since this means we'll be taking the dot product of every input vector with every weight vector, this operation is indistinguishable from a matrix multiplication of the input as a row matrix and the weights as a column matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type Layer end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct DenseLayer <: Layer\n",
    "    # In a dense layer, the neurons and biases are treated as column matrices\n",
    "    neurons::Array\n",
    "    bias::Array\n",
    "    \n",
    "    # Activation function for the layer and its first derivative\n",
    "    Ï•::Function\n",
    "    âˆ‡Ï•::Function\n",
    "    \n",
    "    # Batch states for backprop\n",
    "    input::Array   # Output of the previous layer\n",
    "    net::Array     # input * neurons + bias\n",
    "    output::Array  # activation(net)\n",
    "      \n",
    "    function DenseLayer(input_dim::Int, output_dim::Int, Ï•::Function) where T<:Real\n",
    "        neurons = randn(input_dim, output_dim)\n",
    "        bias = randn(1, output_dim)\n",
    "        return new(neurons, bias, Ï•, gradient(Ï•))\n",
    "    end\n",
    "    \n",
    "    function DenseLayer(neurons::Array{T}, bias::Array{T}, Ï•::Function) where T<:Real\n",
    "        return new(neurons, bias, Ï•, gradient(Ï•))\n",
    "    end\n",
    "\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Output Layers\n",
    "The only real difference between a dense hidden layer and a dense output layer is that the output layer's neurons have a zero bias. I might consolidate `DenseHiddenLayer` and `DenseOutputLayer` in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct DenseOutputLayer <: DenseLayer\n",
    "    # In a dense layer, the neurons and biases are treated as column matrices\n",
    "    neurons::Array\n",
    "    # Even though the output layer has no bias, I'm including it to simplify forwardpass!()\n",
    "    bias::Array\n",
    "    \n",
    "    # Activation function for the layer and its first derivative\n",
    "    Ï•::Function\n",
    "    âˆ‡Ï•::Function\n",
    "    \n",
    "    # Batch states for backprop\n",
    "    input::Array   # Output of the previous layer\n",
    "    net::Array     # input * neurons (+ bias, but bias is 0)\n",
    "    output::Array  # activation(net)\n",
    "      \n",
    "    function DenseOutputLayer(input_dim::Int, output_dim::Int, Ï•::Function) where T<:Real\n",
    "        neurons = randn(input_dim, output_dim)\n",
    "        bias = zeros(1, output_dim)\n",
    "        return new(neurons, bias, Ï•, gradient(Ï•))\n",
    "    end\n",
    "    \n",
    "    function DenseOutputLayer(neurons::Array{T}, bias::Array{T}, Ï•::Function) where T<:Real\n",
    "        return new(neurons, bias, Ï•, gradient(Ï•))\n",
    "    end\n",
    "\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "function batchnormalize(x::Array{T,N}, Î³::T=1., Î²::T=0.)::Array{T,N} where T<:Real where N\n",
    "    # Î¼ = mean of x\n",
    "    Î¼ = sum(x)/length(x)\n",
    "    # ÏƒÂ² = variance of x\n",
    "    ÏƒÂ² = sum((x .- Î¼).^2)\n",
    "    # Normalize x\n",
    "    xÌ‚ = (x .- Î¼) ./ sqrt(ÏƒÂ²)\n",
    "    # Scale and shift\n",
    "    y = xÌ‚ .* Î³ .+ Î²\n",
    "    return y\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "function batchnormalize!(layer::T, Î³=1., Î²=0.) where T<:DenseLayer\n",
    "    layer.output = batchnormalize(layer.output, Î³, Î²)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type NeuralNetwork end\n",
    "abstract type FeedForwardNeuralNetwork <: NeuralNetwork end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct FullyConnectedNeuralNetwork <: FeedForwardNeuralNetwork\n",
    "    layers::Array{Layer}\n",
    "    Î·::Float64\n",
    "    \n",
    "    loss::Function\n",
    "    âˆ‡loss::Function\n",
    "    \n",
    "    function FullyConnectedNeuralNetwork(input_dim::Int, hidden_dims::Vector{Int}, output_dim::Int, Ï•::Vector{Function}, loss::Function, Î·=0.01)\n",
    "        layers = []\n",
    "        \n",
    "        push!(layers, DenseHiddenLayer(input_dim, hidden_dims[1], Ï•[1]))\n",
    "        \n",
    "        for i in 1:length(hidden_dims)-1\n",
    "            push!(layers, DenseHiddenLayer(hidden_dims[i], hidden_dims[i+1], Ï•[i+1]))\n",
    "        end\n",
    "        \n",
    "        push!(layers, DenseOutputLayer(hidden_dims[end], output_dim, Ï•[end]))\n",
    "        \n",
    "        return new(layers, Î·, loss, gradient(loss))\n",
    "    end\n",
    "    \n",
    "    function FullyConnectedNeuralNetwork(layers::Vector{T}, loss::Function, Î·=0.01) where T<:Layer\n",
    "        return new(layers, Î·, loss, gradient(loss))\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "function predict(model::FullyConnectedNeuralNetwork, data)\n",
    "    for layer in model.layers\n",
    "        data = data * layer.neurons .+ layer.bias\n",
    "        data = layer.Ï•.(data)\n",
    "    end\n",
    "    return data[:,1]\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "function train!(model::FullyConnectedNeuralNetwork, data, target, epochs=1, clear=true)\n",
    "    for i in 1:epochs\n",
    "        forwardpass!(model, data)\n",
    "        backprop!(model, target)\n",
    "    end\n",
    "    if clear\n",
    "        for i in 1:length(model.layers)\n",
    "            model.layers[i].input = []\n",
    "            model.layers[i].net = []\n",
    "            model.layers[i].output = []\n",
    "        end\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "function forwardpass!(model::FullyConnectedNeuralNetwork, data::Array{T}) where T<:Real\n",
    "    for layer in model.layers \n",
    "        layer.input = data\n",
    "        layer.net = data * layer.neurons .+ layer.bias\n",
    "        layer.output = layer.Ï•.(layer.net)\n",
    "        data = layer.output\n",
    "    end\n",
    "end;        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "function backprop!(model::FullyConnectedNeuralNetwork, target::Array{T}) where T<:Real   \n",
    "    # Backpropagate error by iteratively updating error delta terms Î´\n",
    "    # Work backwards from output layer\n",
    "    layer = model.layers[end]\n",
    "    # w:   weights\n",
    "    # o:   output\n",
    "    # net: output before activation\n",
    "    # E:   error\n",
    "    # Calculate partial derivative of error with respect to each weight\n",
    "    # âˆ‚Eâ•±âˆ‚wáµ¢â±¼ = âˆ‚Eâ•±âˆ‚oâ±¼ * âˆ‚oâ±¼â•±âˆ‚netâ±¼ * âˆ‚netâ±¼â•±âˆ‚wáµ¢â±¼\n",
    "    # Partial derivative of loss\n",
    "    ðœ•ð¸â•±ðœ•ð‘œ = model.âˆ‡loss.(layer.output, target)\n",
    "    # Partial derivative of activated output\n",
    "    ðœ•ð‘œâ•±ðœ•ð‘›ð‘’ð‘¡ = layer.âˆ‡Ï•.(layer.net)\n",
    "    # Î´=âˆ‚Eâ•±âˆ‚net\n",
    "    # Error with respect to net -- the error terms\n",
    "    ð›¿ = ðœ•ð¸â•±ðœ•ð‘œ .* ðœ•ð‘œâ•±ðœ•ð‘›ð‘’ð‘¡\n",
    "    # âˆ‚netâ•±âˆ‚w is equal to the transpose of the previous layer's output (https://bit.ly/backproperror)\n",
    "    ðœ•ð‘›ð‘’ð‘¡â•±ðœ•ð‘¤ = layer.input'\n",
    "    # Calculate delta terms for the neurons and adjust by the learning rate\n",
    "    ðœ‚ = model.Î·\n",
    "    ð›¥ð‘¤ = -ðœ‚ * ðœ•ð‘›ð‘’ð‘¡â•±ðœ•ð‘¤ * ð›¿\n",
    "    # Update the weights of the output layer\n",
    "    layer.neurons += ð›¥ð‘¤\n",
    "    # Output layer has no bias, so no need to update it\n",
    "    # Now do the rest of the layers in reverse order\n",
    "    for L in length(model.layers)-1:-1:1\n",
    "        layer = model.layers[L]\n",
    "        # Need to calculate weight adjustment, Î”wá´¸\n",
    "        # Î”wá´¸ = -Î· * (oá´¸â»Â¹)áµ€ * Î´á´¸\n",
    "        # Make sure to save error terms Î´á´¸ for backprop\n",
    "        # Î´á´¸ = Î´á´¸âºÂ¹ * (wá´¸âºÂ¹)áµ€ * âˆ‡Ï•á´¸(oá´¸â»Â¹wá´¸)\n",
    "        # Term oá´¸â»Â¹wá´¸ is layer L's unactivated output and stored as netá´¸\n",
    "        # All together\n",
    "        # Î”wá´¸ = -Î· * (oá´¸â»Â¹)áµ€ * Î´á´¸âºÂ¹ * (wá´¸âºÂ¹)áµ€ * âˆ‡Ï•á´¸(oá´¸â»Â¹wá´¸)\n",
    "        ðœ•ð¸â•±ðœ•ð‘œ = ð›¿ * model.layers[L+1].neurons'\n",
    "        ðœ•ð‘œâ•±ðœ•ð‘›ð‘’ð‘¡ = layer.âˆ‡Ï•.(layer.net)\n",
    "        ð›¿ = ðœ•ð¸â•±ðœ•ð‘œ .* ðœ•ð‘œâ•±ðœ•ð‘›ð‘’ð‘¡\n",
    "        ðœ•ð‘›ð‘’ð‘¡â•±ðœ•ð‘¤ = layer.input' \n",
    "        ð›¥ð‘¤ = -ðœ‚ * ðœ•ð‘›ð‘’ð‘¡â•±ðœ•ð‘¤ * ð›¿\n",
    "        # Update the neurons\n",
    "        layer.neurons += ð›¥ð‘¤\n",
    "        # Update the bias by adding scaled error terms\n",
    "        layer.bias = layer.bias .+ (-ðœ‚ * ð›¿)\n",
    "    end   \n",
    "end; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "function fit!(model::FullyConnectedNeuralNetwork, data::Array{T}, target::Vector{T}, epochs::Int, verbose=false) where T<:Real    \n",
    "    if verbose\n",
    "        prediction = predict(model, data)\n",
    "        @show loss(prediction, target)\n",
    "        print(\"Training for \", epochs, \" epochs.\")\n",
    "        @time train!(model, data, target, epochs)\n",
    "        prediction = predict(model, data)\n",
    "        @show loss(prediction, target)\n",
    "    else\n",
    "        train!(model, data, target, epochs)\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type ActivationFunction <: Function end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "âˆ‡sigmoid (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Activations (Ï•)\n",
    "function ReLU(x::T)::T where T<:Real \n",
    "    return max(x, 0)\n",
    "end\n",
    "\n",
    "function LeakyReLU(x::T)::T where T<:Real \n",
    "    return max(x, 0.01x)\n",
    "end\n",
    "\n",
    "function sigmoid(x::T)::T where T<:Real \n",
    "    return 1.0 / (1 + exp(-x))\n",
    "end\n",
    "\n",
    "# Derivatives\n",
    "function âˆ‡ReLU(x::T)::T where T<:Real \n",
    "    return x > 0\n",
    "end\n",
    "\n",
    "function âˆ‡LeakyReLU(x::T)::T where T<:Real \n",
    "    return x < 0 ? 0.01 : 1.0\n",
    "end\n",
    "\n",
    "function âˆ‡sigmoid(x::T)::T where T<:Real\n",
    "    y = sigmoid(x)\n",
    "    return y * (1 - y)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Calculations\n",
    "\n",
    "# Mean Squared Error\n",
    "function mse(x::T, target::T)::T where T<:Real\n",
    "    return .5(target-x)^2\n",
    "end\n",
    "\n",
    "function mse(xs::Vector{T}, target::T)::Vector{T} where T<:Real \n",
    "    err(x) = target - x\n",
    "    return sum(err.(xs).^2)/2*length(xs)\n",
    "end\n",
    "\n",
    "function mse(xs::Vector{T}, target::Vector{T})::Vector{T} where T<:Real\n",
    "    sum((xs .- target).^2)/2*length(xs)\n",
    "end\n",
    "\n",
    "# Derivatives\n",
    "function âˆ‡mse(x::T, target::T)::T where T<:Real\n",
    "    return x - target\n",
    "end\n",
    "\n",
    "function âˆ‡mse(xs::Vector{T}, target::T)::Vector{T} where T<:Real\n",
    "    return xs .- target\n",
    "end\n",
    "\n",
    "function âˆ‡mse(xs::Vector{T}, target::Vector{T})::Vector{T} where T<:Real\n",
    "    return xs .- target\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "function gradient(f::Function)\n",
    "    if f == ReLU\n",
    "        âˆ‡f = âˆ‡ReLU\n",
    "    elseif f == LeakyReLU\n",
    "        âˆ‡f = âˆ‡LeakyReLU\n",
    "    elseif f == sigmoid\n",
    "        âˆ‡f = âˆ‡sigmoid\n",
    "    elseif f == mse\n",
    "        âˆ‡f = âˆ‡mse\n",
    "    elseif f == sin\n",
    "        âˆ‡f = cos\n",
    "    end\n",
    "    \n",
    "    return âˆ‡f\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 8\n",
    "\n",
    "Láµ¢ = DenseHiddenLayer(input_size, 8, LeakyReLU)\n",
    "Lâ‚‚ = DenseHiddenLayer(8, 4, LeakyReLU)\n",
    "Lâ‚ƒ = DenseHiddenLayer(4, 4, LeakyReLU)\n",
    "Lâ‚’ = DenseOutputLayer(4, 1, LeakyReLU)\n",
    "\n",
    "Layers = [Láµ¢, Lâ‚‚, Lâ‚ƒ, Lâ‚’]\n",
    "m = FullyConnectedNeuralNetwork(Layers, mse);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100-element Array{Float64,1}:\n",
       " -0.8818400733415471\n",
       " -1.0115394091213126\n",
       " -2.8013374582850488\n",
       " -1.1338479624557778\n",
       " -1.5543542394313208\n",
       " -0.5008845891822699\n",
       "  0.621397536432255\n",
       " -0.40015056111015096\n",
       " -0.08519886785153656\n",
       " -0.8197982845182499\n",
       " -1.2818157254048503\n",
       " -1.4137913712385337\n",
       "  0.15935307526706213\n",
       "  â‹®\n",
       "  0.8088045246236898\n",
       " -0.36022918508522883\n",
       "  1.8467087091264482\n",
       "  0.621071543330813\n",
       " -0.8669394102684208\n",
       " -1.0241410996668574\n",
       "  4.323153859818787\n",
       " -0.0365663396605443\n",
       " -0.3465276766904928\n",
       " -0.10806592803819598\n",
       "  0.3490531461385517\n",
       "  1.9548198201993763"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples=100\n",
    "data = randn(samples,input_size)\n",
    "target = randn(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss(prediction, target) = 4.008682854974479e125\n",
      "Training for 1000 epochs.  0.052626 seconds (96.01 k allocations: 127.961 MiB, 13.65% gc time)\n",
      "loss(prediction, target) = NaN\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NaN"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(m, data, target, 1000, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss(prediction, target) = 9212.429768515163\n",
      "Training for 100 epochs. 15.483893 seconds (9.51 k allocations: 25.495 GiB, 14.38% gc time)\n",
      "loss(prediction, target) = 178.1208186822642\n"
     ]
    }
   ],
   "source": [
    "# Can we overfit a disporportionately large model on a random matrix?\n",
    "inputsize = 8\n",
    "hidden_layers = [4096,4096]\n",
    "output_size = 1\n",
    "activations = Function[sigmoid, sigmoid, LeakyReLU, LeakyReLU]\n",
    "loss=mse;\n",
    "\n",
    "m = FullyConnectedNeuralNetwork(inputsize, hidden_layers, output_size, activations, loss);\n",
    "\n",
    "samples = 8\n",
    "v = randn(samples, inputsize)\n",
    "t = rand(samples)\n",
    "\n",
    "fit!(m, v, t, 100, true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Float64,1}:\n",
       " 3.3787401227584963\n",
       " 0.29291124116189465\n",
       " 0.2788540073004317\n",
       " 1.9853819220928786\n",
       " 0.793288264236646"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randn(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.1",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
