{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network from Scratch\n",
    "No libraries. 😎\n",
    "\n",
    "WIP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Layers\n",
    "The most basic type of layer in artificial neural networks is the fully connected, or dense, layer in which every neuron in one layer is connected to every neuron in the next layer. Since this means we'll be taking the dot product of every input vector with every weight vector, this operation is indistinguishable from a matrix multiplication of the input as a row matrix and the weights as a column matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type Layer end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct DenseLayer <: Layer\n",
    "    # In a dense layer, the neurons and biases are treated as column matrices\n",
    "    neurons::Array\n",
    "    bias::Array\n",
    "    \n",
    "    # Activation function for the layer and its first derivative\n",
    "    ϕ::Function\n",
    "    ∇ϕ::Function\n",
    "    \n",
    "    # Batch states for backprop\n",
    "    input::Array   # Output of the previous layer\n",
    "    net::Array     # input * neurons + bias\n",
    "    output::Array  # activation(net)\n",
    "      \n",
    "    function DenseLayer(input_dim::Int, output_dim::Int, ϕ::Function) where T<:Real\n",
    "        neurons = randn(input_dim, output_dim)\n",
    "        bias = randn(1, output_dim)\n",
    "        return new(neurons, bias, ϕ, gradient(ϕ))\n",
    "    end\n",
    "    \n",
    "    function DenseLayer(neurons::Array{T}, bias::Array{T}, ϕ::Function) where T<:Real\n",
    "        return new(neurons, bias, ϕ, gradient(ϕ))\n",
    "    end\n",
    "\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Output Layers\n",
    "The only real difference between a dense hidden layer and a dense output layer is that the output layer's neurons have a zero bias. I might consolidate `DenseHiddenLayer` and `DenseOutputLayer` in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct DenseOutputLayer <: DenseLayer\n",
    "    # In a dense layer, the neurons and biases are treated as column matrices\n",
    "    neurons::Array\n",
    "    # Even though the output layer has no bias, I'm including it to simplify forwardpass!()\n",
    "    bias::Array\n",
    "    \n",
    "    # Activation function for the layer and its first derivative\n",
    "    ϕ::Function\n",
    "    ∇ϕ::Function\n",
    "    \n",
    "    # Batch states for backprop\n",
    "    input::Array   # Output of the previous layer\n",
    "    net::Array     # input * neurons (+ bias, but bias is 0)\n",
    "    output::Array  # activation(net)\n",
    "      \n",
    "    function DenseOutputLayer(input_dim::Int, output_dim::Int, ϕ::Function) where T<:Real\n",
    "        neurons = randn(input_dim, output_dim)\n",
    "        bias = zeros(1, output_dim)\n",
    "        return new(neurons, bias, ϕ, gradient(ϕ))\n",
    "    end\n",
    "    \n",
    "    function DenseOutputLayer(neurons::Array{T}, bias::Array{T}, ϕ::Function) where T<:Real\n",
    "        return new(neurons, bias, ϕ, gradient(ϕ))\n",
    "    end\n",
    "\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "function batchnormalize(x::Array{T,N}, γ::T=1., β::T=0.)::Array{T,N} where T<:Real where N\n",
    "    # μ = mean of x\n",
    "    μ = sum(x)/length(x)\n",
    "    # σ² = variance of x\n",
    "    σ² = sum((x .- μ).^2)\n",
    "    # Normalize x\n",
    "    x̂ = (x .- μ) ./ sqrt(σ²)\n",
    "    # Scale and shift\n",
    "    y = x̂ .* γ .+ β\n",
    "    return y\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "function batchnormalize!(layer::T, γ=1., β=0.) where T<:DenseLayer\n",
    "    layer.output = batchnormalize(layer.output, γ, β)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type NeuralNetwork end\n",
    "abstract type FeedForwardNeuralNetwork <: NeuralNetwork end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct FullyConnectedNeuralNetwork <: FeedForwardNeuralNetwork\n",
    "    layers::Array{Layer}\n",
    "    η::Float64\n",
    "    \n",
    "    loss::Function\n",
    "    ∇loss::Function\n",
    "    \n",
    "    function FullyConnectedNeuralNetwork(input_dim::Int, hidden_dims::Vector{Int}, output_dim::Int, ϕ::Vector{Function}, loss::Function, η=0.01)\n",
    "        layers = []\n",
    "        \n",
    "        push!(layers, DenseHiddenLayer(input_dim, hidden_dims[1], ϕ[1]))\n",
    "        \n",
    "        for i in 1:length(hidden_dims)-1\n",
    "            push!(layers, DenseHiddenLayer(hidden_dims[i], hidden_dims[i+1], ϕ[i+1]))\n",
    "        end\n",
    "        \n",
    "        push!(layers, DenseOutputLayer(hidden_dims[end], output_dim, ϕ[end]))\n",
    "        \n",
    "        return new(layers, η, loss, gradient(loss))\n",
    "    end\n",
    "    \n",
    "    function FullyConnectedNeuralNetwork(layers::Vector{T}, loss::Function, η=0.01) where T<:Layer\n",
    "        return new(layers, η, loss, gradient(loss))\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "function predict(model::FullyConnectedNeuralNetwork, data)\n",
    "    for layer in model.layers\n",
    "        data = data * layer.neurons .+ layer.bias\n",
    "        data = layer.ϕ.(data)\n",
    "    end\n",
    "    return data[:,1]\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "function train!(model::FullyConnectedNeuralNetwork, data, target, epochs=1, clear=true)\n",
    "    for i in 1:epochs\n",
    "        forwardpass!(model, data)\n",
    "        backprop!(model, target)\n",
    "    end\n",
    "    if clear\n",
    "        for i in 1:length(model.layers)\n",
    "            model.layers[i].input = []\n",
    "            model.layers[i].net = []\n",
    "            model.layers[i].output = []\n",
    "        end\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "function forwardpass!(model::FullyConnectedNeuralNetwork, data::Array{T}) where T<:Real\n",
    "    for layer in model.layers \n",
    "        layer.input = data\n",
    "        layer.net = data * layer.neurons .+ layer.bias\n",
    "        layer.output = layer.ϕ.(layer.net)\n",
    "        data = layer.output\n",
    "    end\n",
    "end;        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "function backprop!(model::FullyConnectedNeuralNetwork, target::Array{T}) where T<:Real   \n",
    "    # Backpropagate error by iteratively updating error delta terms δ\n",
    "    # Work backwards from output layer\n",
    "    layer = model.layers[end]\n",
    "    # w:   weights\n",
    "    # o:   output\n",
    "    # net: output before activation\n",
    "    # E:   error\n",
    "    # Calculate partial derivative of error with respect to each weight\n",
    "    # ∂E╱∂wᵢⱼ = ∂E╱∂oⱼ * ∂oⱼ╱∂netⱼ * ∂netⱼ╱∂wᵢⱼ\n",
    "    # Partial derivative of loss\n",
    "    𝜕𝐸╱𝜕𝑜 = model.∇loss.(layer.output, target)\n",
    "    # Partial derivative of activated output\n",
    "    𝜕𝑜╱𝜕𝑛𝑒𝑡 = layer.∇ϕ.(layer.net)\n",
    "    # δ=∂E╱∂net\n",
    "    # Error with respect to net -- the error terms\n",
    "    𝛿 = 𝜕𝐸╱𝜕𝑜 .* 𝜕𝑜╱𝜕𝑛𝑒𝑡\n",
    "    # ∂net╱∂w is equal to the transpose of the previous layer's output (https://bit.ly/backproperror)\n",
    "    𝜕𝑛𝑒𝑡╱𝜕𝑤 = layer.input'\n",
    "    # Calculate delta terms for the neurons and adjust by the learning rate\n",
    "    𝜂 = model.η\n",
    "    𝛥𝑤 = -𝜂 * 𝜕𝑛𝑒𝑡╱𝜕𝑤 * 𝛿\n",
    "    # Update the weights of the output layer\n",
    "    layer.neurons += 𝛥𝑤\n",
    "    # Output layer has no bias, so no need to update it\n",
    "    # Now do the rest of the layers in reverse order\n",
    "    for L in length(model.layers)-1:-1:1\n",
    "        layer = model.layers[L]\n",
    "        # Need to calculate weight adjustment, Δwᴸ\n",
    "        # Δwᴸ = -η * (oᴸ⁻¹)ᵀ * δᴸ\n",
    "        # Make sure to save error terms δᴸ for backprop\n",
    "        # δᴸ = δᴸ⁺¹ * (wᴸ⁺¹)ᵀ * ∇ϕᴸ(oᴸ⁻¹wᴸ)\n",
    "        # Term oᴸ⁻¹wᴸ is layer L's unactivated output and stored as netᴸ\n",
    "        # All together\n",
    "        # Δwᴸ = -η * (oᴸ⁻¹)ᵀ * δᴸ⁺¹ * (wᴸ⁺¹)ᵀ * ∇ϕᴸ(oᴸ⁻¹wᴸ)\n",
    "        𝜕𝐸╱𝜕𝑜 = 𝛿 * model.layers[L+1].neurons'\n",
    "        𝜕𝑜╱𝜕𝑛𝑒𝑡 = layer.∇ϕ.(layer.net)\n",
    "        𝛿 = 𝜕𝐸╱𝜕𝑜 .* 𝜕𝑜╱𝜕𝑛𝑒𝑡\n",
    "        𝜕𝑛𝑒𝑡╱𝜕𝑤 = layer.input' \n",
    "        𝛥𝑤 = -𝜂 * 𝜕𝑛𝑒𝑡╱𝜕𝑤 * 𝛿\n",
    "        # Update the neurons\n",
    "        layer.neurons += 𝛥𝑤\n",
    "        # Update the bias by adding scaled error terms\n",
    "        layer.bias = layer.bias .+ (-𝜂 * 𝛿)\n",
    "    end   \n",
    "end; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "function fit!(model::FullyConnectedNeuralNetwork, data::Array{T}, target::Vector{T}, epochs::Int, verbose=false) where T<:Real    \n",
    "    if verbose\n",
    "        prediction = predict(model, data)\n",
    "        @show loss(prediction, target)\n",
    "        print(\"Training for \", epochs, \" epochs.\")\n",
    "        @time train!(model, data, target, epochs)\n",
    "        prediction = predict(model, data)\n",
    "        @show loss(prediction, target)\n",
    "    else\n",
    "        train!(model, data, target, epochs)\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type ActivationFunction <: Function end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "∇sigmoid (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Activations (ϕ)\n",
    "function ReLU(x::T)::T where T<:Real \n",
    "    return max(x, 0)\n",
    "end\n",
    "\n",
    "function LeakyReLU(x::T)::T where T<:Real \n",
    "    return max(x, 0.01x)\n",
    "end\n",
    "\n",
    "function sigmoid(x::T)::T where T<:Real \n",
    "    return 1.0 / (1 + exp(-x))\n",
    "end\n",
    "\n",
    "# Derivatives\n",
    "function ∇ReLU(x::T)::T where T<:Real \n",
    "    return x > 0\n",
    "end\n",
    "\n",
    "function ∇LeakyReLU(x::T)::T where T<:Real \n",
    "    return x < 0 ? 0.01 : 1.0\n",
    "end\n",
    "\n",
    "function ∇sigmoid(x::T)::T where T<:Real\n",
    "    y = sigmoid(x)\n",
    "    return y * (1 - y)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Calculations\n",
    "\n",
    "# Mean Squared Error\n",
    "function mse(x::T, target::T)::T where T<:Real\n",
    "    return .5(target-x)^2\n",
    "end\n",
    "\n",
    "function mse(xs::Vector{T}, target::T)::Vector{T} where T<:Real \n",
    "    err(x) = target - x\n",
    "    return sum(err.(xs).^2)/2*length(xs)\n",
    "end\n",
    "\n",
    "function mse(xs::Vector{T}, target::Vector{T})::Vector{T} where T<:Real\n",
    "    sum((xs .- target).^2)/2*length(xs)\n",
    "end\n",
    "\n",
    "# Derivatives\n",
    "function ∇mse(x::T, target::T)::T where T<:Real\n",
    "    return x - target\n",
    "end\n",
    "\n",
    "function ∇mse(xs::Vector{T}, target::T)::Vector{T} where T<:Real\n",
    "    return xs .- target\n",
    "end\n",
    "\n",
    "function ∇mse(xs::Vector{T}, target::Vector{T})::Vector{T} where T<:Real\n",
    "    return xs .- target\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "function gradient(f::Function)\n",
    "    if f == ReLU\n",
    "        ∇f = ∇ReLU\n",
    "    elseif f == LeakyReLU\n",
    "        ∇f = ∇LeakyReLU\n",
    "    elseif f == sigmoid\n",
    "        ∇f = ∇sigmoid\n",
    "    elseif f == mse\n",
    "        ∇f = ∇mse\n",
    "    elseif f == sin\n",
    "        ∇f = cos\n",
    "    end\n",
    "    \n",
    "    return ∇f\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 8\n",
    "\n",
    "Lᵢ = DenseHiddenLayer(input_size, 8, LeakyReLU)\n",
    "L₂ = DenseHiddenLayer(8, 4, LeakyReLU)\n",
    "L₃ = DenseHiddenLayer(4, 4, LeakyReLU)\n",
    "Lₒ = DenseOutputLayer(4, 1, LeakyReLU)\n",
    "\n",
    "Layers = [Lᵢ, L₂, L₃, Lₒ]\n",
    "m = FullyConnectedNeuralNetwork(Layers, mse);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100-element Array{Float64,1}:\n",
       " -0.8818400733415471\n",
       " -1.0115394091213126\n",
       " -2.8013374582850488\n",
       " -1.1338479624557778\n",
       " -1.5543542394313208\n",
       " -0.5008845891822699\n",
       "  0.621397536432255\n",
       " -0.40015056111015096\n",
       " -0.08519886785153656\n",
       " -0.8197982845182499\n",
       " -1.2818157254048503\n",
       " -1.4137913712385337\n",
       "  0.15935307526706213\n",
       "  ⋮\n",
       "  0.8088045246236898\n",
       " -0.36022918508522883\n",
       "  1.8467087091264482\n",
       "  0.621071543330813\n",
       " -0.8669394102684208\n",
       " -1.0241410996668574\n",
       "  4.323153859818787\n",
       " -0.0365663396605443\n",
       " -0.3465276766904928\n",
       " -0.10806592803819598\n",
       "  0.3490531461385517\n",
       "  1.9548198201993763"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples=100\n",
    "data = randn(samples,input_size)\n",
    "target = randn(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss(prediction, target) = 4.008682854974479e125\n",
      "Training for 1000 epochs.  0.052626 seconds (96.01 k allocations: 127.961 MiB, 13.65% gc time)\n",
      "loss(prediction, target) = NaN\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NaN"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(m, data, target, 1000, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss(prediction, target) = 9212.429768515163\n",
      "Training for 100 epochs. 15.483893 seconds (9.51 k allocations: 25.495 GiB, 14.38% gc time)\n",
      "loss(prediction, target) = 178.1208186822642\n"
     ]
    }
   ],
   "source": [
    "# Can we overfit a disporportionately large model on a random matrix?\n",
    "inputsize = 8\n",
    "hidden_layers = [4096,4096]\n",
    "output_size = 1\n",
    "activations = Function[sigmoid, sigmoid, LeakyReLU, LeakyReLU]\n",
    "loss=mse;\n",
    "\n",
    "m = FullyConnectedNeuralNetwork(inputsize, hidden_layers, output_size, activations, loss);\n",
    "\n",
    "samples = 8\n",
    "v = randn(samples, inputsize)\n",
    "t = rand(samples)\n",
    "\n",
    "fit!(m, v, t, 100, true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Float64,1}:\n",
       " 3.3787401227584963\n",
       " 0.29291124116189465\n",
       " 0.2788540073004317\n",
       " 1.9853819220928786\n",
       " 0.793288264236646"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randn(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.1",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
