{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network from Scratch\n",
    "No libraries. 😎\n",
    "\n",
    "WIP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Layers\n",
    "The most basic type of layer in artificial neural networks is the fully connected, or dense, layer in which every neuron in one layer is connected to every neuron in the next layer. Since this means we'll be taking the dot product of every input vector with every weight vector, this operation is indistinguishable from a matrix multiplication of the input as a row matrix and the weights as a column matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type Layer end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct DenseLayer <: Layer\n",
    "    # In a dense layer, the neurons and biases are treated as column matrices\n",
    "    neurons::Array\n",
    "    bias::Array\n",
    "    \n",
    "    # Activation function for the layer and its first derivative\n",
    "    ϕ::Function\n",
    "    ∇ϕ::Function\n",
    "    \n",
    "    # Batch states for backprop\n",
    "    input::Array   # Output of the previous layer\n",
    "    net::Array     # input * neurons + bias\n",
    "    output::Array  # activation(net)\n",
    "      \n",
    "    function DenseLayer(input_dim::Int, output_dim::Int, ϕ::Function)\n",
    "        neurons = randn(input_dim, output_dim)\n",
    "        bias = randn(1, output_dim)\n",
    "        return new(neurons, bias, ϕ, gradient(ϕ))\n",
    "    end\n",
    "    \n",
    "    function DenseLayer(neurons::Array{T}, bias::Array{T}, ϕ::Function) where T<:Real\n",
    "        return new(neurons, bias, ϕ, gradient(ϕ))\n",
    "    end\n",
    "\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Output Layers\n",
    "The only real difference between a dense hidden layer and a dense output layer is that the output layer's neurons have a zero bias. They used to be separate structs but now they're not. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseOutputLayer"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Returns an instance of DenseLayer with zeroed out bias.\n",
    "\"\"\"\n",
    "function DenseOutputLayer(input_dim::Int, output_dim::Int, ϕ::Function)::DenseLayer\n",
    "    L = DenseLayer(input_dim, output_dim, ϕ)\n",
    "    L.bias = zeros(1, output_dim)\n",
    "    return L\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "function batchnormalize(x::Array{T,N}, γ::T=1., β::T=0.)::Array{T,N} where T<:Real where N\n",
    "    # μ = mean of x\n",
    "    μ = sum(x)/length(x)\n",
    "    # σ² = variance of x\n",
    "    σ² = sum((x .- μ).^2)\n",
    "    # Normalize x\n",
    "    x̂ = (x .- μ) ./ sqrt(σ²)\n",
    "    # Scale and shift\n",
    "    y = x̂ .* γ .+ β\n",
    "    return y\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "function batchnormalize!(layer::T, γ=1., β=0.) where T<:Layer\n",
    "    layer.output = batchnormalize(layer.output, γ, β)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type NeuralNetwork end\n",
    "abstract type FeedForwardNeuralNetwork <: NeuralNetwork end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct FullyConnectedNeuralNetwork <: FeedForwardNeuralNetwork\n",
    "    layers::Array{Layer}\n",
    "    η::Float64\n",
    "    \n",
    "    loss::Function\n",
    "    ∇loss::Function\n",
    "    \n",
    "    function FullyConnectedNeuralNetwork(input_dim::Int, hidden_dims::Vector{Int}, output_dim::Int, ϕ::Vector{Function}, loss::Function, η=0.01)\n",
    "        layers = []\n",
    "        \n",
    "        push!(layers, DenseLayer(input_dim, hidden_dims[1], ϕ[1]))\n",
    "        \n",
    "        for i in 1:length(hidden_dims)-1\n",
    "            push!(layers, DenseLayer(hidden_dims[i], hidden_dims[i+1], ϕ[i+1]))\n",
    "        end\n",
    "        \n",
    "        push!(layers, DenseOutputLayer(hidden_dims[end], output_dim, ϕ[end]))\n",
    "        \n",
    "        return new(layers, η, loss, gradient(loss))\n",
    "    end\n",
    "    \n",
    "    function FullyConnectedNeuralNetwork(layers::Vector{T}, loss::Function, η=0.01) where T<:Layer\n",
    "        return new(layers, η, loss, gradient(loss))\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "function predict(model::FullyConnectedNeuralNetwork, data)\n",
    "    for layer in model.layers\n",
    "        data = data * layer.neurons .+ layer.bias\n",
    "        data = layer.ϕ.(data)\n",
    "    end\n",
    "    return data[:,1]\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "function train!(model::FullyConnectedNeuralNetwork, data, target, epochs=1, clear=true)\n",
    "    for i in 1:epochs\n",
    "        forwardpass!(model, data)\n",
    "        backprop!(model, target)\n",
    "    end\n",
    "    if clear\n",
    "        for i in 1:length(model.layers)\n",
    "            model.layers[i].input = []\n",
    "            model.layers[i].net = []\n",
    "            model.layers[i].output = []\n",
    "        end\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "function forwardpass!(model::FullyConnectedNeuralNetwork, data::Array{T}) where T<:Real\n",
    "    for layer in model.layers \n",
    "        layer.input = data\n",
    "        layer.net = data * layer.neurons .+ layer.bias\n",
    "        layer.output = layer.ϕ.(layer.net)\n",
    "        data = layer.output\n",
    "    end\n",
    "end;        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "function backprop!(model::FullyConnectedNeuralNetwork, target::Array{T}) where T<:Real   \n",
    "    # Backpropagate error by iteratively updating error delta terms δ\n",
    "    # Work backwards from output layer\n",
    "    layer = model.layers[end]\n",
    "    # w:   weights\n",
    "    # o:   output\n",
    "    # net: output before activation\n",
    "    # E:   error\n",
    "    # Calculate partial derivative of error with respect to each weight\n",
    "    # ∂E╱∂wᵢⱼ = ∂E╱∂oⱼ * ∂oⱼ╱∂netⱼ * ∂netⱼ╱∂wᵢⱼ\n",
    "    # Partial derivative of loss\n",
    "    𝜕𝐸╱𝜕𝑜 = model.∇loss.(layer.output, target)\n",
    "    # Partial derivative of activated output\n",
    "    𝜕𝑜╱𝜕𝑛𝑒𝑡 = layer.∇ϕ.(layer.net)\n",
    "    # δ=∂E╱∂net\n",
    "    # Error with respect to net -- the error terms\n",
    "    𝛿 = 𝜕𝐸╱𝜕𝑜 .* 𝜕𝑜╱𝜕𝑛𝑒𝑡\n",
    "    # ∂net╱∂w is equal to the transpose of the previous layer's output (https://bit.ly/backproperror)\n",
    "    𝜕𝑛𝑒𝑡╱𝜕𝑤 = layer.input'\n",
    "    # Calculate delta terms for the neurons and adjust by the learning rate\n",
    "    𝜂 = model.η\n",
    "    𝛥𝑤 = -𝜂 * 𝜕𝑛𝑒𝑡╱𝜕𝑤 * 𝛿\n",
    "    # Update the weights of the output layer\n",
    "    layer.neurons += 𝛥𝑤\n",
    "    # Output layer has no bias, so no need to update it\n",
    "    # Now do the rest of the layers in reverse order\n",
    "    for L in length(model.layers)-1:-1:1\n",
    "        layer = model.layers[L]\n",
    "        # Need to calculate weight adjustment, Δwᴸ\n",
    "        # Δwᴸ = -η * (oᴸ⁻¹)ᵀ * δᴸ\n",
    "        # Make sure to save error terms δᴸ for backprop\n",
    "        # δᴸ = δᴸ⁺¹ * (wᴸ⁺¹)ᵀ * ∇ϕᴸ(oᴸ⁻¹wᴸ)\n",
    "        # Term oᴸ⁻¹wᴸ is layer L's unactivated output and stored as netᴸ\n",
    "        # All together\n",
    "        # Δwᴸ = -η * (oᴸ⁻¹)ᵀ * δᴸ⁺¹ * (wᴸ⁺¹)ᵀ * ∇ϕᴸ(oᴸ⁻¹wᴸ)\n",
    "        𝜕𝐸╱𝜕𝑜 = 𝛿 * model.layers[L+1].neurons'\n",
    "        𝜕𝑜╱𝜕𝑛𝑒𝑡 = layer.∇ϕ.(layer.net)\n",
    "        𝛿 = 𝜕𝐸╱𝜕𝑜 .* 𝜕𝑜╱𝜕𝑛𝑒𝑡\n",
    "        𝜕𝑛𝑒𝑡╱𝜕𝑤 = layer.input' \n",
    "        𝛥𝑤 = -𝜂 * 𝜕𝑛𝑒𝑡╱𝜕𝑤 * 𝛿\n",
    "        # Update the neurons\n",
    "        layer.neurons += 𝛥𝑤\n",
    "        # Update the bias by adding scaled error terms\n",
    "        layer.bias = layer.bias .+ (-𝜂 * 𝛿)\n",
    "    end   \n",
    "end; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "function fit!(model::FullyConnectedNeuralNetwork, data::Array{T}, target::Vector{T}, epochs::Int, verbose=false) where T<:Real    \n",
    "    if verbose\n",
    "        prediction = predict(model, data)\n",
    "        loss = model.loss\n",
    "        @show loss(prediction, target)\n",
    "        print(\"Training for \", epochs, \" epochs.\")\n",
    "        @time train!(model, data, target, epochs)\n",
    "        prediction = predict(model, data)\n",
    "        @show loss(prediction, target)\n",
    "    else\n",
    "        train!(model, data, target, epochs)\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "If a model is only comprised of linear operations such as matrix multiplication, it can only model linear phenomena. To get around this limitation we run the results of our linear operations through some non-linear operation, collectively referred to in this context as *activation functions* in reference to the activation threshold of biological neurons. Because for backprop we need to compute derivatives, it's generally advisable to go with activation functions that have easily computable derivatives.\n",
    "\n",
    "### ReLU\n",
    "A popular activation function for hidden layers is the Rectified Linear Unit, or ReLU, in which if the value is negative it's replaced with zero. This is clearly non-linear, trivial to compute, and has a derivative that's trivial to compute and can be predefined. This has some issues (though it still works in many cases), first and foremost being that all values below zero are collapsed to the same value. A minor tweak, named LeakyReLU, addresses this by dividing negative values by 100 instead of zeroing them out completely. Other variations exist that I'm not getting into here yet (PReLU, ELU, etc).\n",
    "\n",
    "### Sigmoid\n",
    "Explain the squishy of sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "∇sigmoid (generic function with 1 method)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Activations (ϕ)\n",
    "function ReLU(x::T)::T where T<:Real \n",
    "    return max(x, 0)\n",
    "end\n",
    "\n",
    "function LeakyReLU(x::T)::T where T<:Real \n",
    "    return max(x, 0.01x)\n",
    "end\n",
    "\n",
    "function sigmoid(x::T)::T where T<:Real \n",
    "    return 1.0 / (1 + exp(-x))\n",
    "end\n",
    "\n",
    "# Derivatives (∇ϕ)\n",
    "function ∇ReLU(x::T)::T where T<:Real \n",
    "    return x > 0\n",
    "end\n",
    "\n",
    "function ∇LeakyReLU(x::T)::T where T<:Real \n",
    "    return x < 0 ? 0.01 : 1.0\n",
    "end\n",
    "\n",
    "function ∇sigmoid(x::T)::T where T<:Real\n",
    "    y = sigmoid(x)\n",
    "    return y * (1 - y)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Calculations\n",
    "\n",
    "# Mean Squared Error\n",
    "function mse(x::T, target::T)::T where T<:Real\n",
    "    return .5(target-x)^2\n",
    "end\n",
    "\n",
    "function mse(xs::Vector{T}, target::T)::T where T<:Real \n",
    "    err(x) = target - x\n",
    "    return sum(err.(xs).^2)/2*length(xs)\n",
    "end\n",
    "\n",
    "function mse(xs::Vector{T}, target::Vector{T})::T where T<:Real\n",
    "    sum((xs .- target).^2)/2*length(xs)\n",
    "end\n",
    "\n",
    "# Derivatives\n",
    "function ∇mse(x::T, target::T)::T where T<:Real\n",
    "    return x - target\n",
    "end\n",
    "\n",
    "function ∇mse(xs::Vector{T}, target::T)::Vector{T} where T<:Real\n",
    "    return xs .- target\n",
    "end\n",
    "\n",
    "function ∇mse(xs::Vector{T}, target::Vector{T})::Vector{T} where T<:Real\n",
    "    return xs .- target\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "function gradient(f::Function)::Function\n",
    "    # Activation\n",
    "    if f == ReLU\n",
    "        ∇f = ∇ReLU\n",
    "    elseif f == LeakyReLU\n",
    "        ∇f = ∇LeakyReLU\n",
    "    elseif f == sigmoid\n",
    "        ∇f = ∇sigmoid\n",
    "    elseif f == sin\n",
    "        ∇f = cos\n",
    "    \n",
    "    # Loss\n",
    "    elseif f == mse\n",
    "        ∇f = ∇mse\n",
    "    \n",
    "    end\n",
    "    \n",
    "    return ∇f\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss(prediction, target) = 2.1577619616481356\n",
      "Training for 1000 epochs.  0.505927 seconds (1.85 M allocations: 96.296 MiB, 3.46% gc time)\n",
      "loss(prediction, target) = 1.1576305697571925\n"
     ]
    }
   ],
   "source": [
    "input_size = 4\n",
    "\n",
    "Lᵢ = DenseLayer(input_size, 8, LeakyReLU)\n",
    "L₂ = DenseLayer(8, 4, sigmoid)\n",
    "L₃ = DenseLayer(4, 4, sigmoid)\n",
    "Lₒ = DenseOutputLayer(4, 1, sigmoid)\n",
    "\n",
    "Layers = [Lᵢ, L₂, L₃, Lₒ]\n",
    "m = FullyConnectedNeuralNetwork(Layers, mse)\n",
    "\n",
    "samples=4\n",
    "data = randn(samples,input_size)\n",
    "target = rand([0.,1.], samples)\n",
    "fit!(m, data, target, 1000, true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss(prediction, target) = 2.2041308033103513\n",
      "Training for 10000 epochs.  0.497362 seconds (3.21 M allocations: 365.298 MiB, 6.27% gc time)\n",
      "loss(prediction, target) = 1.0029765415713756\n"
     ]
    }
   ],
   "source": [
    "# Can we overfit a disporportionately large model on a random matrix?\n",
    "inputsize = 4\n",
    "hidden_layers = [8,8,8,8,8,4,4,4,2,2,2,1]\n",
    "output_size = 1\n",
    "activations = vcat(repeat([LeakyReLU], length(hidden_layers)+1), sigmoid)\n",
    "loss=mse;\n",
    "\n",
    "model = FullyConnectedNeuralNetwork(inputsize, hidden_layers, output_size, activations, loss);\n",
    "\n",
    "samples = 4\n",
    "data = randn(samples, inputsize)\n",
    "target = rand([0.,1.], samples)\n",
    "\n",
    "fit!(model, data, target, 10000, true);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.1",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
