{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type Layer end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic type of layer in artificial neural networks is the densely connected layer, in which every neuron in one layer is connected to every neuron in the next layer. Since this means we'll be taking the dot product of every input vector with every weight vector, this operation is indistinguishable from a matrix multiplication of the input as a row matrix and the weights as a column matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct DenseLayer <: Layer\n",
    "    neurons::Array\n",
    "    bias::Array\n",
    "    \n",
    "    Ï•::Function\n",
    "    âˆ‡Ï•::Function\n",
    "    \n",
    "    input::Array\n",
    "    net::Array\n",
    "    output::Array\n",
    "      \n",
    "    function DenseLayer(input_dim::Int, output_dim::Int, Ï•::Function)\n",
    "        neurons = randn(input_dim, output_dim)\n",
    "        bias = randn(1, output_dim)\n",
    "        return new(neurons, bias, Ï•, gradient(Ï•))\n",
    "    end\n",
    "    \n",
    "    function DenseLayer(neurons::Array, bias::Vector, Ï•::Function)\n",
    "        return new(neurons, bias, Ï•, gradient(Ï•))\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct DenseOutputLayer <: Layer\n",
    "    neurons::Array\n",
    "    bias::Array\n",
    "    \n",
    "    Ï•::Function\n",
    "    âˆ‡Ï•::Function\n",
    "    \n",
    "    input::Array\n",
    "    net::Array\n",
    "    output::Array\n",
    "      \n",
    "    function DenseOutputLayer(input_dim::Int, output_dim::Int, Ï•::Function)\n",
    "        neurons = randn(input_dim, output_dim)\n",
    "        bias = zeros(1, output_dim)\n",
    "        return new(neurons, bias, Ï•, gradient(Ï•))\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type NeuralNetwork end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct FeedForwardNeuralNetwork <: NeuralNetwork\n",
    "    layers::Array{Layer}\n",
    "    Î·::Float64\n",
    "    \n",
    "    loss::Function\n",
    "    âˆ‡loss::Function\n",
    "    \n",
    "    function FeedForwardNeuralNetwork(input_dim::Int, hidden_dims::Vector, output_dim::Int, Ï•::Vector{Function}, loss::Function, Î·=0.01)\n",
    "        layers = []\n",
    "        \n",
    "        push!(layers, DenseLayer(input_dim, hidden_dims[1], Ï•[1]))\n",
    "        \n",
    "        for i in 1:length(hidden_dims)-1\n",
    "            push!(layers, DenseLayer(hidden_dims[i], hidden_dims[i+1], Ï•[i+1]))\n",
    "        end\n",
    "        \n",
    "        push!(layers, DenseOutputLayer(hidden_dims[end], output_dim, Ï•[end]))\n",
    "        \n",
    "        return new(layers, Î·, loss, gradient(loss))\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "function predict(model::FeedForwardNeuralNetwork, data)\n",
    "    for layer in model.layers\n",
    "        data = data * layer.neurons .+ layer.bias\n",
    "        data = layer.Ï•.(data)\n",
    "    end\n",
    "    return data[:,1]\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "function train(model::FeedForwardNeuralNetwork, data, target, epochs=1, clear=true)\n",
    "    for i in 1:epochs\n",
    "        forwardpass(model, data)\n",
    "        backprop(model, target)\n",
    "    end\n",
    "    if clear\n",
    "        for i in 1:length(model.layers)\n",
    "            model.layers[i].input = []\n",
    "            model.layers[i].net = []\n",
    "            model.layers[i].output = []\n",
    "        end\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "function forwardpass(model::FeedForwardNeuralNetwork, data)\n",
    "    for layer in model.layers \n",
    "        layer.input = data\n",
    "        layer.net = data * layer.neurons .+ layer.bias\n",
    "        layer.output = layer.Ï•.(layer.net)\n",
    "        data = layer.output\n",
    "    end\n",
    "    return data\n",
    "end;        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "function backprop(model::FeedForwardNeuralNetwork, target)    \n",
    "    # Backpropagate error by iteratively updating error delta terms Î´\n",
    "    # Work backwards from output layer\n",
    "    layer = model.layers[end]\n",
    "    # w:   weights\n",
    "    # o:   output\n",
    "    # net: output before activation\n",
    "    # E:   error\n",
    "    # Calculate partial derivative of error with respect to each weight\n",
    "    # âˆ‚Eâ•±âˆ‚wáµ¢â±¼ = âˆ‚Eâ•±âˆ‚oâ±¼ * âˆ‚oâ±¼â•±âˆ‚netâ±¼ * âˆ‚netâ±¼â•±âˆ‚wáµ¢â±¼\n",
    "    # Partial derivative of loss\n",
    "    ðœ•ð¸â•±ðœ•ð‘œ = model.âˆ‡loss.(layer.output, target)\n",
    "    # Partial derivative of activated output\n",
    "    ðœ•ð‘œâ•±ðœ•ð‘›ð‘’ð‘¡ = layer.âˆ‡Ï•.(layer.net)\n",
    "    # Î´=âˆ‚Eâ•±âˆ‚net\n",
    "    # Error with respect to net -- the error terms\n",
    "    ð›¿ = ðœ•ð¸â•±ðœ•ð‘œ .* ðœ•ð‘œâ•±ðœ•ð‘›ð‘’ð‘¡\n",
    "    # âˆ‚netâ•±âˆ‚w is equal to the transpose of the previous layer's output (https://bit.ly/backproperror)\n",
    "    ðœ•ð‘›ð‘’ð‘¡â•±ðœ•ð‘¤ = layer.input'\n",
    "    # Calculate delta terms for the neurons and adjust by the learning rate\n",
    "    ðœ‚ = model.Î·\n",
    "    ð›¥ð‘¤ = -ðœ‚ * ðœ•ð‘›ð‘’ð‘¡â•±ðœ•ð‘¤ * ð›¿\n",
    "    # Update the weights of the output layer\n",
    "    layer.neurons += ð›¥ð‘¤\n",
    "    # Output layer has no bias, so no need to update it\n",
    "    # Now do the rest of the layers in reverse order\n",
    "    for L in length(model.layers)-1:-1:1\n",
    "        layer = model.layers[L]\n",
    "        # Need to calculate weight adjustment, Î”wá´¸\n",
    "        # Î”wá´¸ = -Î· * (oá´¸â»Â¹)áµ€ * Î´á´¸\n",
    "        # Make sure to save error terms Î´á´¸ for backprop\n",
    "        # Î´á´¸ = Î´á´¸âºÂ¹ * (wá´¸âºÂ¹)áµ€ * âˆ‡Ï•á´¸(oá´¸â»Â¹wá´¸)\n",
    "        # Term oá´¸â»Â¹wá´¸ is layer L's unactivated output and stored as netá´¸\n",
    "        # All together\n",
    "        # Î”wá´¸ = -Î· * (oá´¸â»Â¹)áµ€ * Î´á´¸âºÂ¹ * (wá´¸âºÂ¹)áµ€ * âˆ‡Ï•á´¸(oá´¸â»Â¹wá´¸)\n",
    "        ðœ•ð¸â•±ðœ•ð‘œ = ð›¿ * model.layers[L+1].neurons'\n",
    "        ðœ•ð‘œâ•±ðœ•ð‘›ð‘’ð‘¡ = layer.âˆ‡Ï•.(layer.net)\n",
    "        ð›¿ = ðœ•ð¸â•±ðœ•ð‘œ .* ðœ•ð‘œâ•±ðœ•ð‘›ð‘’ð‘¡\n",
    "        ðœ•ð‘›ð‘’ð‘¡â•±ðœ•ð‘¤ = layer.input' \n",
    "        ð›¥ð‘¤ = -ðœ‚ * ðœ•ð‘›ð‘’ð‘¡â•±ðœ•ð‘¤ * ð›¿\n",
    "        # Update the neurons\n",
    "        layer.neurons += ð›¥ð‘¤\n",
    "        # Update the bias by adding scaled error terms\n",
    "        layer.bias = layer.bias .+ (-ðœ‚ * ð›¿)\n",
    "    end   \n",
    "end; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "function fit(model::FeedForwardNeuralNetwork, data::Array{T}, target::Vector{T}, epochs::Int, verbose=false) where T<:Real    \n",
    "    if verbose\n",
    "        prediction = predict(model, data)\n",
    "        @show loss(prediction, target)\n",
    "        print(\"Training for \", epochs, \" epochs.\")\n",
    "        @time train(model, data, target, epochs)\n",
    "        prediction = predict(model, data)\n",
    "        @show loss(prediction, target)\n",
    "    else\n",
    "        train(model, data, target, epochs)\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activations (Ï•)\n",
    "function ReLU(x::T)::T where T<:Real \n",
    "    return max(x, 0)\n",
    "end\n",
    "\n",
    "function LeakyReLU(x::T)::T where T<:Real \n",
    "    return max(x, 0.01x)\n",
    "end\n",
    "\n",
    "function sigmoid(x::T)::T where T<:Real \n",
    "    return 1.0 / (1 + exp(-x))\n",
    "end\n",
    "\n",
    "# Derivatives\n",
    "function âˆ‡ReLU(x::T)::T where T<:Real \n",
    "    return x > 0\n",
    "end\n",
    "\n",
    "function âˆ‡LeakyReLU(x::T)::T where T<:Real \n",
    "    return x < 0 ? 0.01 : 1.0\n",
    "end\n",
    "\n",
    "function âˆ‡sigmoid(x::T)::T where T<:Real\n",
    "    y = sigmoid(x)\n",
    "    return y * (1 - y)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Calculations\n",
    "\n",
    "# Mean Squared Error\n",
    "function mse(x::T, target::T) where T<:Real\n",
    "    return .5(target-x)^2\n",
    "end\n",
    "\n",
    "function mse(xs::Vector{T}, target::T) where T<:Real \n",
    "    err(x) = target - x\n",
    "    return sum(err.(xs).^2)/2*length(xs)\n",
    "end\n",
    "\n",
    "function mse(xs::Vector{T}, target::Vector{T}) where T<:Real\n",
    "    sum((xs .- target).^2)/2*length(xs)\n",
    "end\n",
    "\n",
    "# Derivatives\n",
    "function âˆ‡mse(x::T, target::T) where T<:Real\n",
    "    return x - target\n",
    "end\n",
    "\n",
    "function âˆ‡mse(xs::Vector{T}, target::T) where T<:Real\n",
    "    return xs .- target\n",
    "end\n",
    "\n",
    "function âˆ‡mse(xs::Vector{T}, target::Vector{T}) where T<:Real\n",
    "    return xs .- target\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "function gradient(f::Function)\n",
    "    if f == ReLU\n",
    "        âˆ‡f = âˆ‡ReLU\n",
    "    elseif f == LeakyReLU\n",
    "        âˆ‡f = âˆ‡LeakyReLU\n",
    "    elseif f == sigmoid\n",
    "        âˆ‡f = âˆ‡sigmoid\n",
    "    elseif f == mse\n",
    "        âˆ‡f = âˆ‡mse\n",
    "    end\n",
    "    \n",
    "    return âˆ‡f\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss(prediction, target) = 2.0382464111116865\n",
      "Training for 10000 epochs.  5.140711 seconds (4.78 M allocations: 6.523 GiB, 13.24% gc time)\n",
      "loss(prediction, target) = 0.08744189721527532\n"
     ]
    }
   ],
   "source": [
    "# Can we overfit a disporportionately large model on a random matrix?\n",
    "inputsize = 4\n",
    "hidden_layers = [128,128,64,64,32,32,16,16,8,4,2]\n",
    "output_size = 1\n",
    "activations = vcat([LeakyReLU],repeat([sigmoid], length(hidden_layers)+1))\n",
    "loss=mse;\n",
    "\n",
    "m = FeedForwardNeuralNetwork(inputsize, hidden_layers, output_size, activations, loss);\n",
    "\n",
    "samples = 4\n",
    "v = randn(samples, inputsize)\n",
    "t = rand([0.,1.], samples)\n",
    "\n",
    "fit(m, v, t, 10000, true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.1",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
