{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type Layer end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic type of layer in artificial neural networks is the densely connected layer, in which every neuron in one layer is connected to every neuron in the next layer. Since this means we'll be taking the dot product of every input vector with every weight vector, this operation is indistinguishable from a matrix multiplication of the input as a row matrix and the weights as a column matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct DenseLayer <: Layer\n",
    "    neurons::Array\n",
    "    bias::Array\n",
    "    \n",
    "    ϕ::Function\n",
    "    ∇ϕ::Function\n",
    "    \n",
    "    input::Array\n",
    "    net::Array\n",
    "    output::Array\n",
    "      \n",
    "    function DenseLayer(input_dim::Int, output_dim::Int, ϕ::Function)\n",
    "        neurons = randn(input_dim, output_dim)\n",
    "        bias = randn(1, output_dim)\n",
    "        return new(neurons, bias, ϕ, gradient(ϕ))\n",
    "    end\n",
    "    \n",
    "    function DenseLayer(neurons::Array, bias::Vector, ϕ::Function)\n",
    "        return new(neurons, bias, ϕ, gradient(ϕ))\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct DenseOutputLayer <: Layer\n",
    "    neurons::Array\n",
    "    bias::Array\n",
    "    \n",
    "    ϕ::Function\n",
    "    ∇ϕ::Function\n",
    "    \n",
    "    input::Array\n",
    "    net::Array\n",
    "    output::Array\n",
    "      \n",
    "    function DenseOutputLayer(input_dim::Int, output_dim::Int, ϕ::Function)\n",
    "        neurons = randn(input_dim, output_dim)\n",
    "        bias = zeros(1, output_dim)\n",
    "        return new(neurons, bias, ϕ, gradient(ϕ))\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type NeuralNetwork end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct FeedForwardNeuralNetwork <: NeuralNetwork\n",
    "    layers::Array{Layer}\n",
    "    η::Float64\n",
    "    \n",
    "    loss::Function\n",
    "    ∇loss::Function\n",
    "    \n",
    "    function FeedForwardNeuralNetwork(input_dim::Int, hidden_dims::Vector, output_dim::Int, ϕ::Vector{Function}, loss::Function, η=0.01)\n",
    "        layers = []\n",
    "        \n",
    "        push!(layers, DenseLayer(input_dim, hidden_dims[1], ϕ[1]))\n",
    "        \n",
    "        for i in 1:length(hidden_dims)-1\n",
    "            push!(layers, DenseLayer(hidden_dims[i], hidden_dims[i+1], ϕ[i+1]))\n",
    "        end\n",
    "        \n",
    "        push!(layers, DenseOutputLayer(hidden_dims[end], output_dim, ϕ[end]))\n",
    "        \n",
    "        return new(layers, η, loss, gradient(loss))\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "function predict(model::FeedForwardNeuralNetwork, data)\n",
    "    for layer in model.layers\n",
    "        data = data * layer.neurons .+ layer.bias\n",
    "        data = layer.ϕ.(data)\n",
    "    end\n",
    "    return data[:,1]\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "function train(model::FeedForwardNeuralNetwork, data, target, epochs=1, clear=true)\n",
    "    for i in 1:epochs\n",
    "        forwardpass(model, data)\n",
    "        backprop(model, target)\n",
    "    end\n",
    "    if clear\n",
    "        for i in 1:length(model.layers)\n",
    "            model.layers[i].input = []\n",
    "            model.layers[i].net = []\n",
    "            model.layers[i].output = []\n",
    "        end\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "function forwardpass(model::FeedForwardNeuralNetwork, data)\n",
    "    for layer in model.layers \n",
    "        layer.input = data\n",
    "        layer.net = data * layer.neurons .+ layer.bias\n",
    "        layer.output = layer.ϕ.(layer.net)\n",
    "        data = layer.output\n",
    "    end\n",
    "    return data\n",
    "end;        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "function backprop(model::FeedForwardNeuralNetwork, target)    \n",
    "    # Backpropagate error by iteratively updating error delta terms δ\n",
    "    # Work backwards from output layer\n",
    "    layer = model.layers[end]\n",
    "    # w:   weights\n",
    "    # o:   output\n",
    "    # net: output before activation\n",
    "    # E:   error\n",
    "    # Calculate partial derivative of error with respect to each weight\n",
    "    # ∂E/∂wᵢⱼ = ∂E/∂oⱼ * ∂oⱼ/∂netⱼ * ∂netⱼ/∂wᵢⱼ\n",
    "    # Partial derivative of loss\n",
    "    ∂E_∂o = model.∇loss.(layer.output, target)\n",
    "    # Partial derivative of activated output\n",
    "    ∂o_∂net = layer.∇ϕ.(layer.net)\n",
    "    # δ=∂E/∂net\n",
    "    # Error with respect to net -- the error terms\n",
    "    δ = ∂E_∂o .* ∂o_∂net\n",
    "    # ∂net/∂w is equal to the previous layer's output (https://bit.ly/backproperror)\n",
    "    ∂net_∂w = layer.input'\n",
    "    # Calculate delta terms for the neurons and adjust by the learning rate\n",
    "    η = model.η\n",
    "    Δw = -η * ∂net_∂w * δ\n",
    "    # Update the weights of the output layer\n",
    "    layer.neurons += Δw\n",
    "    # Output layer has no bias, so no need to update it\n",
    "    # Now do the rest of the layers in reverse order\n",
    "    for L in length(model.layers)-1:-1:1\n",
    "        layer = model.layers[L]\n",
    "        # Need to calculate weight adjustment, Δwᴸ\n",
    "        # Δwᴸ = -η * (oᴸ⁻¹)ᵀ * δᴸ\n",
    "        # Make sure to save error terms δᴸ for backprop\n",
    "        # δᴸ = δᴸ⁺¹ * (wᴸ⁺¹)ᵀ * ∇ϕᴸ(oᴸ⁻¹wᴸ)\n",
    "        # Term oᴸ⁻¹wᴸ is layer L's unactivated output and stored as netᴸ\n",
    "        # All together\n",
    "        # Δwᴸ = -η * (oᴸ⁻¹)ᵀ * δᴸ⁺¹ * (wᴸ⁺¹)ᵀ * ∇ϕᴸ(oᴸ⁻¹wᴸ)\n",
    "        ∂E_∂o = δ * model.layers[L+1].neurons'\n",
    "        ∂o_∂net = layer.∇ϕ.(layer.net)\n",
    "        δ = ∂E_∂o .* ∂o_∂net\n",
    "        ∂net_∂w = layer.input' \n",
    "        Δw = -η * ∂net_∂w * δ\n",
    "        # Update the neurons\n",
    "        layer.neurons += Δw\n",
    "        # Update the bias by adding scaled error terms\n",
    "        layer.bias = layer.bias .+ (-η * δ)\n",
    "    end   \n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "function fit(model::FeedForwardNeuralNetwork, data::Array{T}, target::Vector{T}, epochs::Int, verbose=false) where T<:Real    \n",
    "    if verbose\n",
    "        prediction = predict(model, data)\n",
    "        @show loss(prediction, target)\n",
    "        print(\"Training for \", epochs, \" epochs.\")\n",
    "        @time train(model, data, target, epochs)\n",
    "        prediction = predict(model, data)\n",
    "        @show loss(prediction, target)\n",
    "    else\n",
    "        train(model, data, target, epochs)\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activations (ϕ)\n",
    "function ReLU(x::T)::T where T<:Real \n",
    "    return max(x, 0)\n",
    "end\n",
    "\n",
    "function LeakyReLU(x::T)::T where T<:Real \n",
    "    return max(x, 0.01x)\n",
    "end\n",
    "\n",
    "function sigmoid(x::T)::T where T<:Real \n",
    "    return 1.0 / (1 + exp(-x))\n",
    "end\n",
    "\n",
    "# Derivatives\n",
    "function ∇ReLU(x::T)::T where T<:Real \n",
    "    return x > 0\n",
    "end\n",
    "\n",
    "function ∇LeakyReLU(x::T)::T where T<:Real \n",
    "    return x < 0 ? 0.01 : 1.0\n",
    "end\n",
    "\n",
    "function ∇sigmoid(x::T)::T where T<:Real\n",
    "    y = sigmoid(x)\n",
    "    return y * (1 - y)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Calculations\n",
    "\n",
    "# Mean Squared Error\n",
    "function mse(x::T, target::T) where T<:Real\n",
    "    return .5(target-x)^2\n",
    "end\n",
    "\n",
    "function mse(xs::Vector{T}, target::T) where T<:Real \n",
    "    err(x) = target - x\n",
    "    return sum(err.(xs).^2)/2*length(xs)\n",
    "end\n",
    "\n",
    "function mse(xs::Vector{T}, target::Vector{T}) where T<:Real\n",
    "    sum((xs .- target).^2)/2*length(xs)\n",
    "end\n",
    "\n",
    "# Derivatives\n",
    "function ∇mse(x::T, target::T) where T<:Real\n",
    "    return x - target\n",
    "end\n",
    "\n",
    "function ∇mse(xs::Vector{T}, target::T) where T<:Real\n",
    "    return xs .- target\n",
    "end\n",
    "\n",
    "function ∇mse(xs::Vector{T}, target::Vector{T}) where T<:Real\n",
    "    return xs .- target\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "function gradient(f::Function)\n",
    "    if f == ReLU\n",
    "        ∇f = ∇ReLU\n",
    "    elseif f == LeakyReLU\n",
    "        ∇f = ∇LeakyReLU\n",
    "    elseif f == sigmoid\n",
    "        ∇f = ∇sigmoid\n",
    "    elseif f == mse\n",
    "        ∇f = ∇mse\n",
    "    end\n",
    "    \n",
    "    return ∇f\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss(prediction, target) = 12.261282604474928\n",
      "Training for 10000 epochs.  6.079542 seconds (4.78 M allocations: 7.936 GiB, 13.18% gc time)\n",
      "loss(prediction, target) = 0.13156896802248919\n"
     ]
    }
   ],
   "source": [
    "# Can we overfit a disporportionately large model on a random matrix?\n",
    "inputsize = 8\n",
    "hidden_layers = [128,128,64,64,32,32,16,16,8,4,2]\n",
    "output_size = 1\n",
    "activations = vcat([LeakyReLU],repeat([sigmoid], length(hidden_layers)+1))\n",
    "loss=mse;\n",
    "\n",
    "m = FeedForwardNeuralNetwork(inputsize, hidden_layers, output_size, activations, loss);\n",
    "\n",
    "samples = 8\n",
    "v = randn(samples, inputsize)\n",
    "t = rand([0.,1.], samples)\n",
    "\n",
    "fit(m, v, t, 10000, true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.1",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
